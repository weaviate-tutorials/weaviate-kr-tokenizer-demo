{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaviate & keyword search \n",
    "\n",
    "### English: easy\n",
    "\n",
    "Splitting up English sentences into words is easy. \n",
    "\n",
    "For example, the sentence `Hello, beautiful world!` splits up into `[\"Hello\", \"beautiful\", \"world!\"]`. \n",
    "\n",
    "### Korean: not so easy\n",
    "\n",
    "What about this?\n",
    "\n",
    "```\n",
    "아버지가방에들어가신다\n",
    "```\n",
    "\n",
    "Using spaces only, it will not be split up at all:\n",
    "\n",
    "```\n",
    "- [\"아버지가방에들어가신다\"]\n",
    "```\n",
    "\n",
    "And it could easily be wrong like:\n",
    "```\n",
    "- [\"아버지\", \"가방\", \"에\", \"들어가\", \"신다\"] ❌ (Father goes into bag)\n",
    "```\n",
    "\n",
    "It should be:\n",
    "```\n",
    "- [\"아버지\", \"가\", \"방\", \"에\", \"들어가\", \"신다\"] ✅ (Father goes into the room)\n",
    "```\n",
    "\n",
    "Great search is critical for building great AI applications, and the ability to split a sentence into words is a key part of that. \n",
    "\n",
    "### Introducing Weaviate's Korean tokenizer\n",
    "\n",
    "In Weaviate `1.25.7`, we introduce a Korean tokenizer that can split Korean sentences into words. This is a significant step forward in helping Korean developers build great AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with Weaviate\n",
    "\n",
    "Install Docker and run the following command to start a Weaviate instance:\n",
    "\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "Run `pip install weaviate-client` to install the Weaviate client. \n",
    "\n",
    "Then, run the following code to connect to Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721616776.815328 9085742 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import os\n",
    "\n",
    "cohere_key = os.environ[\"COHERE_API_KEY\"]\n",
    "\n",
    "client = weaviate.connect_to_local(\n",
    "    headers={\"X-Cohere-Api-Key\": cohere_key}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection below uses the \"Kagome\" tokenizer with the \"MeCab-ko\" dictionary to tokenize Korean sentences. \n",
    "\n",
    "- [How to set a tokenizer](https://weaviate.io/developers/weaviate/manage-data/collections#property-level-settings)\n",
    "- [Available tokenizers](https://weaviate.io/developers/weaviate/config-refs/schema#tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure, Property, DataType, Tokenization\n",
    "\n",
    "# Delete the collection if it exists\n",
    "if client.collections.exists(\"Wiki\"):\n",
    "    client.collections.delete(\"Wiki\")\n",
    "\n",
    "# Create the collection\n",
    "wiki = client.collections.create(\n",
    "    name=\"Wiki\",\n",
    "    properties=[\n",
    "        Property(\n",
    "            name=\"title\",\n",
    "            data_type=DataType.TEXT,\n",
    "            tokenization=Tokenization.KAGOME_KR\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"chunk\",\n",
    "            data_type=DataType.TEXT,\n",
    "            tokenization=Tokenization.KAGOME_KR\n",
    "        ),\n",
    "    ],\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_cohere(\n",
    "            name=\"chunk\",\n",
    "            source_properties=[\"chunk\"],\n",
    "            model=\"embed-multilingual-v3.0\"\n",
    "        ),\n",
    "    ],\n",
    "    generative_config=Configure.Generative.cohere(model=\"command-r-plus\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper code\n",
    "\n",
    "These functions help us pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load texts (Korean Wikipedia text)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"./data\")\n",
    "src_texts = [\n",
    "    {\"body\": txt_file.read_text(), \"title\": txt_file.stem}\n",
    "    for txt_file in data_dir.glob(\"*.txt\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into small chunks\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def get_chunks_fixed_size(text: str, chunk_size: int) -> List[str]:\n",
    "    overlap = int(chunk_size // 4)\n",
    "    return [text[i:i+chunk_size+overlap] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def get_chunks(text: str) -> List[str]:\n",
    "    sections = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    for s in sections:\n",
    "        if len(s) > 100:\n",
    "            sub_chunks = get_chunks_fixed_size(s, 50)\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append(s)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data into Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n"
     ]
    }
   ],
   "source": [
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "with wiki.batch.fixed_size(batch_size=200) as batch:\n",
    "    for src_text in src_texts:\n",
    "        chunks = get_chunks(src_text[\"body\"])\n",
    "        for chunk in chunks:\n",
    "            batch.add_object(\n",
    "                properties={\n",
    "                    \"title\": src_text[\"title\"],\n",
    "                    \"chunk\": chunk,\n",
    "                },\n",
    "                uuid=generate_uuid5(chunk)\n",
    "            )\n",
    "\n",
    "# Print the total number of imported chunks\n",
    "count = wiki.aggregate.over_all(total_count=True).total_count\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example searches\n",
    "\n",
    "Let's check if this is working properly.\n",
    "\n",
    "### \"머리\" vs \"머리말\" \n",
    "\n",
    "These are very different words in Korean:\n",
    "- \"머리\"  (head)\n",
    "- \"머리말\"  (page header / preface)\n",
    "\n",
    "If \"머리말\" is not tokenized correctly, the search results will include results relating to \"머리\" (head).\n",
    "\n",
    "Let's see what happens if we search for \"머리구성\" (head composition) and \"머리말구성\" (page header / preface composition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Search & translate\n",
    "\n",
    "We perform searches and translate the results using a Cohere large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== RESULTS FOR QUERY: 머리구성 ==========\n",
      "\n",
      "========== RESULT 1 ==========\n",
      "ARTICLE TITLE: skull\n",
      "CHUNK BODY: == 구조 == [[파일:Lateral head skull.jpg|섬네일|왼쪽|머리의 구성]] 머리뼈는 얼굴을 ...\n",
      "TRANSLATION: == Structure == [[File:Lateral head skull.jpg|thumb|left|Structure of the head]] The skull is compos...\n",
      "\n",
      "========== RESULT 2 ==========\n",
      "ARTICLE TITLE: skull\n",
      "CHUNK BODY: ]] 머리뼈는 얼굴을 구성하고 머리뼈공간을 보호한다. [[뇌]]를 비롯하여 [[눈 (해부학)|눈]], [[귀]]...\n",
      "TRANSLATION: The skull forms the face and protects the cranial cavity. It houses the [[brain]], as well as the [[...\n",
      "\n",
      "========== RESULTS FOR QUERY: 머리말구성 ==========\n",
      "\n",
      "========== RESULT 1 ==========\n",
      "ARTICLE TITLE: preface\n",
      "CHUNK BODY: 적으로 머리말을 만들고 유지하는 기능을 제공하며 여기서 머리말은 페이지마다 동일할 수도 있고 페이지 번호와 같이...\n",
      "TRANSLATION: It provides the ability to create and maintain headers as an enemy, where headers can be the same on...\n",
      "\n",
      "========== RESULT 2 ==========\n",
      "ARTICLE TITLE: preface\n",
      "CHUNK BODY: '''머리말''' 또는 '''머리글'''은 [[타이포그래피]]에서 본문과 구별되면서도 인쇄된 페이지의 꼭대기에 ...\n",
      "TRANSLATION: A 'headword' or 'header' in typography is distinct from the main text while being at the top of a pr...\n"
     ]
    }
   ],
   "source": [
    "for query in [\"머리구성\", \"머리말구성\"]:\n",
    "\n",
    "    r = wiki.generate.bm25(\n",
    "        query=query,\n",
    "        query_properties=[\"chunk\"],\n",
    "        single_prompt=\"Return a translation of this into English (and nothing else): {chunk}\",\n",
    "        limit=2\n",
    "    )\n",
    "    print(f\"\\n========== RESULTS FOR QUERY: {query} ==========\")\n",
    "    for i, o in enumerate(r.objects):\n",
    "        print(f\"\\n========== RESULT {i+1} ==========\")\n",
    "        print(\"ARTICLE TITLE:\", o.properties[\"title\"])\n",
    "        print(\"CHUNK BODY:\", o.properties[\"chunk\"].replace(\"\\n\", \" \")[:100] + \"...\")\n",
    "        print(\"TRANSLATION:\", o.generated.replace(\"\\n\", \" \")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Search & summarize\n",
    "\n",
    "We perform searches and summarise the results into Korean and English, using a Cohere large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== RESULTS FOR QUERY: 머리구성 ==========\n",
      "GENERATED SUMMARY:\n",
      "Here is a summary of the findings about the structure of the head in Korean and English: \n",
      "\n",
      "- 머리뼈는 머리뼈공간을 보호하고, 뇌, 눈, 귀를 포함해 얼굴을 구성합니다. - The skull protects the cranial cavity and forms the face, including the brain, eyes, and ears.\n",
      "- 대부분의 좌우 대칭 동물은 머리를 가지고 있으며, 머리는 신체의 앞쪽 끝부분을 구성합니다. - Most bilaterally symmetrical animals have a head, which forms the anterior end of the body.\n",
      "\n",
      "========== RESULTS FOR QUERY: 머리말구성 ==========\n",
      "GENERATED SUMMARY:\n",
      "Here is a summary of the key points about '머리말구성' (preface composition) in both Korean and English:\n",
      "\n",
      "- 머리말은 본문과 구분되면서도 페이지의 꼭대기에 위치하는 인쇄된 텍스트입니다. - The preface is printed text that is distinct from the main body and located at the top of the page.\n",
      "\n",
      "- 머리말은 페이지마다 동일하거나 페이지 번호와 같이 달라질 수 있습니다. - The preface can remain the same or vary on each page, such as with page numbers.\n",
      "\n",
      "- 출판물에서 머리말은 난외표제라고도 불리며, 이는 펼친 책의 왼쪽과 오른쪽 페이지에 나타납니다. - In publishing, the preface is also known as a 'running head', appearing on the left and right pages of an open book.\n"
     ]
    }
   ],
   "source": [
    "for query in [\"머리구성\", \"머리말구성\"]:\n",
    "\n",
    "    r = wiki.generate.bm25(\n",
    "        query=query,\n",
    "        query_properties=[\"chunk\"],\n",
    "        grouped_task=f\"Summarise the findings here into a few bullet points about {query}. Each point should be a single sentence, and in Korean AND English.\",\n",
    "        limit=3\n",
    "    )\n",
    "    print(f\"\\n========== RESULTS FOR QUERY: {query} ==========\")\n",
    "    print(\"GENERATED SUMMARY:\")\n",
    "    print(r.generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
