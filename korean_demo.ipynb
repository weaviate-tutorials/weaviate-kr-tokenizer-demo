{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaviate & Korean\n",
    "\n",
    "![Img](./assets/korean_tokenization_0.png)\n",
    "\n",
    "[Weaviate](https://www.weaviate.io) includes powerful integrations that help you build AI apps with Korean data. \n",
    "\n",
    "This includes integration with multi-lingual models like Cohere's, and now, a Korean tokenizer.\n",
    "\n",
    "## Korean Tokenizer\n",
    "\n",
    "Tokenization splits up text into components, and is critical for performing keyword searches. But tokenization is not as simple as it sounds.\n",
    "\n",
    "### English\n",
    "\n",
    "Splitting up English sentences into words is relatively easy, as you can split on spaces. \n",
    "\n",
    "For example, the sentence `Hello, beautiful world!` splits up into `[\"Hello\", \"beautiful\", \"world!\"]`. \n",
    "\n",
    "### Korean\n",
    "\n",
    "But, Korean is a different story. Korean words do not always have spaces between them. So, splitting up Korean sentences is not as simple. For example, how would you split up this sentence?\n",
    "\n",
    "```\n",
    "아버지가방에들어가신다\n",
    "```\n",
    "\n",
    "Using spaces only, it will not be split up at all:\n",
    "\n",
    "```\n",
    "- [\"아버지가방에들어가신다\"]\n",
    "```\n",
    "\n",
    "Now, search for \"아버지\" (father) will not return this sentence, even though it contains the word \"아버지\".\n",
    "\n",
    "And splits using words can easily be wrong. This uses common Korean words, but the split is incorrect:\n",
    "```\n",
    "- [\"아버지\", \"가방\", \"에\", \"들어가\", \"신다\"] ❌ (Father goes into bag)\n",
    "```\n",
    "\n",
    "It should be:\n",
    "```\n",
    "- [\"아버지\", \"가\", \"방\", \"에\", \"들어가\", \"신다\"] ✅ (Father goes into the room)\n",
    "```\n",
    "\n",
    "![Img](./assets/korean_tokenization_1.png)\n",
    "\n",
    "Great search is critical for building great AI applications, and the ability to split a sentence into words is a key part of that. \n",
    "\n",
    "### Introducing Weaviate's Korean tokenizer\n",
    "\n",
    "In Weaviate `1.25.7`, we introduce a Korean tokenizer that can split Korean sentences into words. This is a significant step forward in helping Korean developers build great AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with Weaviate\n",
    "\n",
    "Install Docker and run the following command to start a Weaviate instance:\n",
    "\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "Run `pip install weaviate-client` to install the Weaviate client. \n",
    "\n",
    "Then, run the following code to connect to Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721674034.418590 9285394 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import os\n",
    "\n",
    "cohere_key = os.environ[\"COHERE_API_KEY\"]\n",
    "\n",
    "client = weaviate.connect_to_local(\n",
    "    headers={\"X-Cohere-Api-Key\": cohere_key}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection below uses the \"Kagome\" tokenizer with the \"MeCab-ko\" dictionary to tokenize Korean sentences. \n",
    "\n",
    "- [How to set a tokenizer](https://weaviate.io/developers/weaviate/manage-data/collections#property-level-settings)\n",
    "- [Available tokenizers](https://weaviate.io/developers/weaviate/config-refs/schema#tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure, Property, DataType, Tokenization\n",
    "\n",
    "# Delete the collection if it exists\n",
    "if client.collections.exists(\"Wiki\"):\n",
    "    client.collections.delete(\"Wiki\")\n",
    "\n",
    "# Create the collection\n",
    "wiki = client.collections.create(\n",
    "    name=\"Wiki\",\n",
    "    properties=[\n",
    "        Property(\n",
    "            name=\"title\",\n",
    "            data_type=DataType.TEXT,\n",
    "            tokenization=Tokenization.KAGOME_KR\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"chunk\",\n",
    "            data_type=DataType.TEXT,\n",
    "            tokenization=Tokenization.KAGOME_KR\n",
    "        ),\n",
    "    ],\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_cohere(\n",
    "            name=\"chunk\",\n",
    "            source_properties=[\"chunk\"],\n",
    "            model=\"embed-multilingual-v3.0\"  # Multi-lingual embedding model\n",
    "        ),\n",
    "    ],\n",
    "    generative_config=Configure.Generative.cohere(model=\"command-r-plus\")  # Multi-lingual large language model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper code\n",
    "\n",
    "These functions help us pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load texts (Korean Wikipedia text)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"./data\")\n",
    "src_texts = [\n",
    "    {\"body\": txt_file.read_text(), \"title\": txt_file.stem}\n",
    "    for txt_file in data_dir.glob(\"*.txt\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into small chunks\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def get_chunks_fixed_size(text: str, chunk_size: int) -> List[str]:\n",
    "    overlap = int(chunk_size // 4)\n",
    "    return [text[i:i+chunk_size+overlap] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def get_chunks(text: str) -> List[str]:\n",
    "    sections = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    for s in sections:\n",
    "        if len(s) > 100:\n",
    "            sub_chunks = get_chunks_fixed_size(s, 50)\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append(s)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data into Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n"
     ]
    }
   ],
   "source": [
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "with wiki.batch.fixed_size(batch_size=200) as batch:\n",
    "    for src_text in src_texts:\n",
    "        chunks = get_chunks(src_text[\"body\"])\n",
    "        for chunk in chunks:\n",
    "            batch.add_object(\n",
    "                properties={\n",
    "                    \"title\": src_text[\"title\"],\n",
    "                    \"chunk\": chunk,\n",
    "                },\n",
    "                uuid=generate_uuid5(chunk)\n",
    "            )\n",
    "\n",
    "# Print the total number of imported chunks\n",
    "count = wiki.aggregate.over_all(total_count=True).total_count\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example keyword searches\n",
    "\n",
    "Let's check if this is working properly by searching with sentences using similar words.\n",
    "\n",
    "These are very different words in Korean:\n",
    "- \"머리\"  (head)\n",
    "- \"머리말\"  (page header / preface)\n",
    "\n",
    "If \"머리말\" is not tokenized correctly, the search results will include results relating to \"머리\" (head).\n",
    "\n",
    "The Korean tokenizer should be able to differentiate between these two words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: `머리` vs `머리말`\n",
    "\n",
    "Let's see what happens if we search for \"머리\" (head) and \"머리말구성\" (page header / preface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== RESULTS FOR QUERY: 머리 ==========\n",
      "\n",
      "========== RESULT 1 ==========\n",
      "ARTICLE TITLE: head\n",
      "CHUNK BODY: 아주 단순한 동물의 경우 머리가 없는 것도 있으나 대부분의 [[좌우 대칭 동물류]]는 머리가 있다. [[척추동물...\n",
      "\n",
      "========== RESULT 2 ==========\n",
      "ARTICLE TITLE: head\n",
      "CHUNK BODY:  머리 그림]] '''머리'''({{llang|en|Head}})는 [[인간]]이나 [[동물]]의 [[목]] 위...\n",
      "\n",
      "========== RESULTS FOR QUERY: 머리말 ==========\n",
      "\n",
      "========== RESULT 1 ==========\n",
      "ARTICLE TITLE: preface\n",
      "CHUNK BODY: 적으로 머리말을 만들고 유지하는 기능을 제공하며 여기서 머리말은 페이지마다 동일할 수도 있고 페이지 번호와 같이...\n",
      "\n",
      "========== RESULT 2 ==========\n",
      "ARTICLE TITLE: preface\n",
      "CHUNK BODY: '''머리말''' 또는 '''머리글'''은 [[타이포그래피]]에서 본문과 구별되면서도 인쇄된 페이지의 꼭대기에 ...\n"
     ]
    }
   ],
   "source": [
    "for query in [\"머리\", \"머리말\"]:\n",
    "\n",
    "    r = wiki.generate.bm25(\n",
    "        query=query,\n",
    "        query_properties=[\"chunk\"],\n",
    "        limit=2\n",
    "    )\n",
    "    print(f\"\\n========== RESULTS FOR QUERY: {query} ==========\")\n",
    "    for i, o in enumerate(r.objects):\n",
    "        print(f\"\\n========== RESULT {i+1} ==========\")\n",
    "        print(\"ARTICLE TITLE:\", o.properties[\"title\"])\n",
    "        print(\"CHUNK BODY:\", o.properties[\"chunk\"].replace(\"\\n\", \" \")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: `머리구성` vs `머리말구성`\n",
    "\n",
    "Let's see what happens if we search for slightly more complex phrases, like: \"머리구성\" (head composition) and \"머리말구성\" (page header / preface composition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== RESULTS FOR QUERY: 머리구성 ==========\n",
      "\n",
      "========== RESULT 1 ==========\n",
      "ARTICLE TITLE: skull\n",
      "CHUNK BODY: == 구조 == [[파일:Lateral head skull.jpg|섬네일|왼쪽|머리의 구성]] 머리뼈는 얼굴을 ...\n",
      "\n",
      "========== RESULT 2 ==========\n",
      "ARTICLE TITLE: skull\n",
      "CHUNK BODY: ]] 머리뼈는 얼굴을 구성하고 머리뼈공간을 보호한다. [[뇌]]를 비롯하여 [[눈 (해부학)|눈]], [[귀]]...\n",
      "\n",
      "========== RESULTS FOR QUERY: 머리말구성 ==========\n",
      "\n",
      "========== RESULT 1 ==========\n",
      "ARTICLE TITLE: preface\n",
      "CHUNK BODY: 적으로 머리말을 만들고 유지하는 기능을 제공하며 여기서 머리말은 페이지마다 동일할 수도 있고 페이지 번호와 같이...\n",
      "\n",
      "========== RESULT 2 ==========\n",
      "ARTICLE TITLE: preface\n",
      "CHUNK BODY: '''머리말''' 또는 '''머리글'''은 [[타이포그래피]]에서 본문과 구별되면서도 인쇄된 페이지의 꼭대기에 ...\n"
     ]
    }
   ],
   "source": [
    "for query in [\"머리구성\", \"머리말구성\"]:\n",
    "\n",
    "    r = wiki.generate.bm25(\n",
    "        query=query,\n",
    "        query_properties=[\"chunk\"],\n",
    "        limit=2\n",
    "    )\n",
    "    print(f\"\\n========== RESULTS FOR QUERY: {query} ==========\")\n",
    "    for i, o in enumerate(r.objects):\n",
    "        print(f\"\\n========== RESULT {i+1} ==========\")\n",
    "        print(\"ARTICLE TITLE:\", o.properties[\"title\"])\n",
    "        print(\"CHUNK BODY:\", o.properties[\"chunk\"].replace(\"\\n\", \" \")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval augmented generation (RAG)\n",
    "\n",
    "Weaviate is AI-native, meaning it integrates with generative AI models to perform retrieval augmented generation. This makes it **easy to build AI applications**.\n",
    "\n",
    "Above, we have set up Weaviate with:\n",
    "\n",
    "- Cohere's multi-lingual embedding model (`embed-multilingual-v3.0`)\n",
    "- Cohere's multi-lingual generative model (`command-r-plus`)\n",
    "\n",
    "So we can perform RAG with Korean data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG example 1: Translate\n",
    "\n",
    "Here, we translate each result into English using the generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== RESULTS FOR QUERY: 머리구성 ==========\n",
      "\n",
      "========== RESULT 1 ==========\n",
      "ARTICLE TITLE: skull\n",
      "CHUNK BODY: == 구조 == [[파일:Lateral head skull.jpg|섬네일|왼쪽|머리의 구성]] 머리뼈는 얼굴을 ...\n",
      "TRANSLATION: == Structure == [[File:Lateral head skull.jpg|thumb|left|Composition of the head]] The skull is comp...\n",
      "\n",
      "========== RESULT 2 ==========\n",
      "ARTICLE TITLE: skull\n",
      "CHUNK BODY: ]] 머리뼈는 얼굴을 구성하고 머리뼈공간을 보호한다. [[뇌]]를 비롯하여 [[눈 (해부학)|눈]], [[귀]]...\n",
      "TRANSLATION: The skull forms the face and protects the cranial cavity. It includes the brain, eyes, and ears....\n",
      "\n",
      "========== RESULTS FOR QUERY: 머리말구성 ==========\n",
      "\n",
      "========== RESULT 1 ==========\n",
      "ARTICLE TITLE: preface\n",
      "CHUNK BODY: 적으로 머리말을 만들고 유지하는 기능을 제공하며 여기서 머리말은 페이지마다 동일할 수도 있고 페이지 번호와 같이...\n",
      "TRANSLATION: It provides the ability to create and maintain headers as an enemy, where the header can be the same...\n",
      "\n",
      "========== RESULT 2 ==========\n",
      "ARTICLE TITLE: preface\n",
      "CHUNK BODY: '''머리말''' 또는 '''머리글'''은 [[타이포그래피]]에서 본문과 구별되면서도 인쇄된 페이지의 꼭대기에 ...\n",
      "TRANSLATION: A 'headword' or 'header' in typography is set apart from the main text while still appearing at the ...\n"
     ]
    }
   ],
   "source": [
    "for query in [\"머리구성\", \"머리말구성\"]:\n",
    "\n",
    "    r = wiki.generate.bm25(\n",
    "        query=query,\n",
    "        query_properties=[\"chunk\"],\n",
    "        single_prompt=\"Return a translation of this into English (and nothing else): {chunk}\",\n",
    "        limit=2\n",
    "    )\n",
    "    print(f\"\\n========== RESULTS FOR QUERY: {query} ==========\")\n",
    "    for i, o in enumerate(r.objects):\n",
    "        print(f\"\\n========== RESULT {i+1} ==========\")\n",
    "        print(\"ARTICLE TITLE:\", o.properties[\"title\"])\n",
    "        print(\"CHUNK BODY:\", o.properties[\"chunk\"].replace(\"\\n\", \" \")[:100] + \"...\")\n",
    "        print(\"TRANSLATION:\", o.generated.replace(\"\\n\", \" \")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG example 2: Search & summarize\n",
    "\n",
    "We use all results with a prompt into one output.\n",
    "\n",
    "Here, we ask the model to write a summary in bullet points, in Korean AND English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== RESULTS FOR QUERY: 머리구성 ==========\n",
      "GENERATED SUMMARY:\n",
      "Here is a summary of the findings about the structure of the head in Korean and English: \n",
      "\n",
      "- 머리뼈는 머리뼈공간을 보호하고, 뇌, 눈, 귀를 포함하는 구조물입니다. - The skull protects the cranial cavity and houses the brain, eyes, and ears.\n",
      "- 머리뼈는 22개의 뼈로 이루어져 있습니다. - The skull is composed of 22 bones.\n",
      "- 머리뼈의 모양은 동물마다 다를 수 있지만, 기본적인 구조는 비슷합니다. - While the shape of the skull can vary between animals, the basic structure remains similar.\n",
      "- 단순한 동물을 제외한 대부분의 좌우 대칭 동물은 머리를 가지고 있습니다. - Most bilateral symmetrical animals, except for very simple ones, have heads.\n",
      "\n",
      "========== RESULTS FOR QUERY: 머리말구성 ==========\n",
      "GENERATED SUMMARY:\n",
      "Here is a summary of the key points about '머리말구성' (preface composition) in both Korean and English:\n",
      "\n",
      "- 머리말은 본문과 구분되면서도 페이지의 꼭대기에 위치하는 타이포그래피 요소입니다. - The preface is a typographical element that is distinct from the main text and located at the top of the page.\n",
      "- 머리말은 페이지마다 동일하거나 페이지 번호와 같이 달라질 수 있습니다. - The preface can remain the same on every page or vary with elements like page numbers.\n",
      "- 출판물에서 머리말은 난외표제라고도 불리며, 이는 펼친 책의 왼쪽과 오른쪽 페이지에 동시에 나타납니다. - In publishing, the preface is also called a \"running head,\" appearing simultaneously on the left and right pages of an open book.\n"
     ]
    }
   ],
   "source": [
    "for query in [\"머리구성\", \"머리말구성\"]:\n",
    "\n",
    "    r = wiki.generate.bm25(\n",
    "        query=query,\n",
    "        query_properties=[\"chunk\"],\n",
    "        grouped_task=f\"Summarise the findings here into a few bullet points about {query}. Each point should be a single sentence, and in Korean AND English.\",\n",
    "        limit=3\n",
    "    )\n",
    "    print(f\"\\n========== RESULTS FOR QUERY: {query} ==========\")\n",
    "    print(\"GENERATED SUMMARY:\")\n",
    "    print(r.generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example semantic searches\n",
    "\n",
    "We can also perform semantic searches (based on meaning) using Weaviate, and hybrid searches that combine the best of both worlds.\n",
    "\n",
    "Because the embedding model is multi-lingual (`embed-multilingual-v3.0`), we can perform searches in Korean and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== RESULTS FOR QUERY: head ==========\n",
      "\n",
      "========== RESULT 1 ==========\n",
      "ARTICLE TITLE: head\n",
      "CHUNK BODY:  머리 그림]] '''머리'''({{llang|en|Head}})는 [[인간]]이나 [[동물]]의 [[목]] 위...\n",
      "TRANSLATION: 'Head' (in English) is the part of the body that is above the neck in humans and animals....\n",
      "\n",
      "========== RESULT 2 ==========\n",
      "ARTICLE TITLE: head\n",
      "CHUNK BODY: 물]]의 [[목]] 위의 부분을 가리킨다. 대개의 경우 머리에는 [[눈 (해부학)|눈]], [[코]], [[입]...\n",
      "TRANSLATION: It refers to the part above the [[neck]] of the [[water]]. In most cases, the head includes [[eye (a...\n",
      "\n",
      "========== RESULTS FOR QUERY: 머리 ==========\n",
      "\n",
      "========== RESULT 1 ==========\n",
      "ARTICLE TITLE: head\n",
      "CHUNK BODY:  머리 그림]] '''머리'''({{llang|en|Head}})는 [[인간]]이나 [[동물]]의 [[목]] 위...\n",
      "TRANSLATION: '''Head''' ({{llang|en|Head}}) is the part of the [[human]] or [[animal]] above the [[neck]]....\n",
      "\n",
      "========== RESULT 2 ==========\n",
      "ARTICLE TITLE: skull\n",
      "CHUNK BODY: 78-89-6109-092-6}}, 215쪽</ref> 머리를 이루는 뼈는 크게 보아 [[뇌머리뼈]], [[얼굴...\n",
      "TRANSLATION: \"78-89-6109-092-6}}, page 215</ref> The bones that make up the head are broadly classified into [[br...\n"
     ]
    }
   ],
   "source": [
    "# Semantic search in Korean & English - shows very similar results\n",
    "for query in [\"head\", \"머리\"]:\n",
    "    r = wiki.generate.near_text(\n",
    "        query=query,\n",
    "        target_vector=\"chunk\",\n",
    "        single_prompt=\"Return a translation of this into English (and nothing else): {chunk}\",\n",
    "        limit=2\n",
    "    )\n",
    "    print(f\"\\n========== RESULTS FOR QUERY: {query} ==========\")\n",
    "    for i, o in enumerate(r.objects):\n",
    "        print(f\"\\n========== RESULT {i+1} ==========\")\n",
    "        print(\"ARTICLE TITLE:\", o.properties[\"title\"])\n",
    "        print(\"CHUNK BODY:\", o.properties[\"chunk\"].replace(\"\\n\", \" \")[:100] + \"...\")\n",
    "        print(\"TRANSLATION:\", o.generated.replace(\"\\n\", \" \")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative feedback loops (GFL)\n",
    "\n",
    "(Preview note) We are building \"generative feedback loop\" tools, which allow you to enrich and enhance your data using these generative outputs. \n",
    "\n",
    "As a basic example, these translated outputs or summarised outputs can be added back into Weaviate, and used going forward.\n",
    "\n",
    "Keep an eye out for this feature in future releases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "Weaviate's Korean tokenizer is a significant step forward in helping Korean developers build great AI applications.\n",
    "\n",
    "Try out Weaviate, starting with the [Quickstart](https://weaviate.io/developers/weaviate/quickstart). \n",
    "\n",
    "And where you have Korean data, set the property tokenizer to \"kagome_kr\" as shown in the code above. \n",
    "\n",
    "### Note\n",
    "\n",
    "- As of `1.25.7`, the tokenizer must be separately enabled by setting `ENABLE_TOKENIZER_KAGOME_KR` [environment variable](https://weaviate.io/developers/weaviate/config-refs/env-vars) to `true`. (For example, in the `docker-compose.yml` file.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
